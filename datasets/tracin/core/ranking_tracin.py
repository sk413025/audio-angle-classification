"""\nTracIn implementation specifically adapted for the ranking task.\n\nThis module extends the base TracIn implementation to work with\nour specific ranking datasets and loss functions.\n"""\n\nimport os\nimport torch\nimport numpy as np\nimport json\nfrom typing import List, Dict, Optional, Tuple, Union\nfrom torch.utils.data import DataLoader, Dataset\n\n# 更新導入路徑以使用新的模組結構\nfrom datasets.tracin.core.tracin import TracInCP, get_default_device\nfrom models.resnet_ranker import SimpleCNNAudioRanker\nfrom losses.ghm_loss import GHMRankingLoss\nfrom torch.nn import MarginRankingLoss\n\n\nclass RankingTracInCP(TracInCP):\n    """\n    TracIn implementation adapted for our audio ranking task.\n    \n    This class handles the specifics of computing influence scores\n    for ranking pairs, working with our project's datasets and models.\n    """\n    \n    def __init__(\n        self,\n        model: SimpleCNNAudioRanker,\n        checkpoints: List[str],\n        loss_type: str = \"standard\",\n        margin: float = 1.0,\n        ghm_bins: int = 10,\n        ghm_alpha: float = 0.75,\n        device: torch.device = None\n    ):\n        """\n        Initialize RankingTracInCP.\n        \n        Args:\n            model: The model architecture (un-trained instance of SimpleCNNAudioRanker)\n            checkpoints: List of file paths to the saved model checkpoints\n            loss_type: Type of loss function ('standard' or 'ghm')\n            margin: Margin parameter for the ranking loss\n            ghm_bins: Number of bins for GHM loss\n            ghm_alpha: Alpha parameter for GHM loss\n            device: Device to perform computations on\n        """\n        # Get device if not provided\n        if device is None:\n            device = get_default_device()\n            \n        # Create loss function based on loss_type\n        if loss_type == \"ghm\":\n            loss_fn = GHMRankingLoss(\n                margin=margin,\n                bins=ghm_bins,\n                alpha=ghm_alpha\n            )\n        else:  # \"standard\"\n            loss_fn = MarginRankingLoss(margin=margin)\n        \n        # Initialize the parent class\n        super().__init__(model, checkpoints, loss_fn, device)\n        \n        self.loss_type = loss_type\n        print(f\"RankingTracInCP initialized with {loss_type} loss on {device}\")\n    \n    def compute_gradients_for_pair(\n        self,\n        x1: torch.Tensor,\n        x2: torch.Tensor,\n        target: torch.Tensor,\n        checkpoint: str\n    ) -> List[torch.Tensor]:\n        """\n        Compute gradients for a ranking pair.\n        \n        Args:\n            x1: First input tensor in the pair\n            x2: Second input tensor in the pair\n            target: Target tensor (1 if x1 > x2, -1 if x1 < x2)\n            checkpoint: Path to the model checkpoint to use\n            \n        Returns:\n            List of gradient tensors for each parameter\n        """\n        # Load the checkpoint\n        checkpoint_data = torch.load(checkpoint, map_location=self.device)\n        self.model.load_state_dict(checkpoint_data['model_state_dict'])\n        self.model.to(self.device)\n        self.model.eval()\n        \n        # Move inputs to device\n        x1 = x1.to(self.device)\n        x2 = x2.to(self.device)\n        target = target.to(self.device)\n        \n        # Reshape target to match scores dimensions if needed\n        # scores shape is typically [batch_size, 1] and target is [batch_size]\n        if target.dim() == 1:\n            target = target.unsqueeze(1)\n        \n        # Zero gradients\n        self.model.zero_grad()\n        \n        # Forward pass\n        scores1 = self.model(x1)\n        scores2 = self.model(x2)\n        \n        # Compute loss\n        if self.loss_type == \"ghm\":\n            # For GHM loss, we need to pass both scores directly\n            loss = self.loss_fn(scores1, scores2, target)\n        else:\n            # For standard ranking loss\n            loss = self.loss_fn(scores1, scores2, target)\n        \n        # Backward pass\n        loss.backward()\n        \n        # Get gradients\n        gradients = []\n        for param in self.model.parameters():\n            if param.requires_grad and param.grad is not None:\n                gradients.append(param.grad.clone().detach())\n                \n        return gradients\n    \n    def compute_influence_for_pair(\n        self,\n        dataset: Dataset,\n        test_x1: torch.Tensor,\n        test_x2: torch.Tensor,\n        test_target: torch.Tensor,\n        batch_size: int = 32,\n        num_workers: int = 4\n    ) -> Tuple[Dict[str, float], Dict[str, Dict[str, float]]]:\n        """\n        Compute the influence scores of all pairs in a dataset on a test pair.\n        \n        Args:\n            dataset: Dataset containing ranking pairs\n            test_x1: First input tensor of the test pair\n            test_x2: Second input tensor of the test pair\n            test_target: Target tensor of the test pair\n            batch_size: Batch size for processing examples\n            num_workers: Number of workers for the DataLoader\n            \n        Returns:\n            Tuple containing:\n            1. Dictionary mapping pair indices to total influence scores\n            2. Dictionary mapping pair indices to per-checkpoint influence scores\n        """\n        # Move test data to device\n        test_x1 = test_x1.to(self.device)\n        test_x2 = test_x2.to(self.device)\n        test_target = test_target.to(self.device)\n        \n        # Create dataloader\n        dataloader = DataLoader(\n            dataset,\n            batch_size=batch_size,\n            shuffle=False,\n            num_workers=num_workers\n        )\n        \n        # Initialize influence scores\n        influence_scores = {}\n        # New dictionary to store per-checkpoint influence scores\n        per_checkpoint_influences = {}\n        \n        # Get checkpoint names (extract from paths)\n        checkpoint_names = [os.path.basename(cp) for cp in self.checkpoints]\n        \n        # Compute gradients for the test pair\n        test_gradients_per_checkpoint = []\n        for checkpoint in self.checkpoints:\n            test_gradients = self.compute_gradients_for_pair(\n                test_x1, test_x2, test_target, checkpoint\n            )\n            test_gradients_per_checkpoint.append(test_gradients)\n       \n        # Process pairs in batches\n        for batch_idx, batch_data in enumerate(dataloader):\n            # Extract batch data - RankingPairDataset returns 7 values\n            x1_batch, x2_batch, targets_batch, _, _, sample_id1_batch, sample_id2_batch = batch_data\n            \n            # Use sample IDs as indices for the influence scores\n            indices = [f\"{id1}_{id2}\" for id1, id2 in zip(sample_id1_batch, sample_id2_batch)]\n            current_batch_size = len(indices)\n            \n            batch_influence = np.zeros(current_batch_size)\n            # New array to store per-checkpoint influence for this batch\n            batch_per_checkpoint_influence = [np.zeros(current_batch_size) for _ in self.checkpoints]\n            \n            # Compute influence across all checkpoints\n            for checkpoint_idx, checkpoint in enumerate(self.checkpoints):\n                # Compute gradients for this batch\n                train_gradients = self.compute_gradients_for_pair(\n                    x1_batch, x2_batch, targets_batch, checkpoint\n                )\n                \n                # Get test gradients for this checkpoint\n                test_gradients = test_gradients_per_checkpoint[checkpoint_idx]\n                \n                # Compute dot product between train and test gradients\n                for i in range(current_batch_size):\n                    # Extract gradients for this sample (assuming batch size 1)\n                    dot_product = 0\n                    for train_grad, test_grad in zip(train_gradients, test_gradients):\n                        dot_product += torch.sum(\n                            train_grad[i].flatten() * test_grad.flatten()\n                        ).cpu().numpy()\n                    \n                    # Add to batch influence\n                    batch_influence[i] += dot_product\n                    \n                    # Store per-checkpoint influence\n                    batch_per_checkpoint_influence[checkpoint_idx][i] = dot_product\n            \n            # Save influence scores\n            for i, idx in enumerate(indices):\n                influence_scores[idx] = float(batch_influence[i])\n                \n                # Save per-checkpoint influence scores\n                per_checkpoint_influences[idx] = {\n                    checkpoint_names[j]: float(batch_per_checkpoint_influence[j][i])\n                    for j in range(len(self.checkpoints))\n                }\n            \n            if batch_idx % 10 == 0:\n                print(f\"Processed {batch_idx+1}/{len(dataloader)} batches\")\n        \n        return influence_scores, per_checkpoint_influences\n    \n    def compute_self_influence_for_pairs(\n        self,\n        dataset: Dataset,\n        batch_size: int = 32,\n        num_workers: int = 4\n    ) -> Tuple[Dict[str, float], Dict[str, Dict[str, float]]]:\n        """\n        Compute the self-influence scores of all pairs in a dataset.\n        \n        Args:\n            dataset: Dataset containing ranking pairs\n            batch_size: Batch size for processing examples\n            num_workers: Number of workers for the DataLoader\n            \n        Returns:\n            Tuple containing:\n            1. Dictionary mapping pair indices to total self-influence scores\n            2. Dictionary mapping pair indices to per-checkpoint self-influence scores\n        """\n        # Create dataloader\n        dataloader = DataLoader(\n            dataset,\n            batch_size=batch_size,\n            shuffle=False,\n            num_workers=num_workers\n        )\n        \n        # Initialize self-influence scores\n        self_influence_scores = {}\n        per_checkpoint_influences = {}\n        \n        # Get checkpoint names (extract from paths)\n        checkpoint_names = [os.path.basename(cp) for cp in self.checkpoints]\n        \n        # Process pairs in batches\n        for batch_idx, batch_data in enumerate(dataloader):\n            # Extract batch data - RankingPairDataset returns 7 values\n            x1_batch, x2_batch, targets_batch, _, _, sample_id1_batch, sample_id2_batch = batch_data\n            \n            # Use sample IDs as indices for the self-influence scores\n            indices = [f\"{id1}_{id2}\" for id1, id2 in zip(sample_id1_batch, sample_id2_batch)]\n            current_batch_size = len(indices)\n            \n            batch_self_influence = np.zeros(current_batch_size)\n            batch_per_checkpoint_influence = [np.zeros(current_batch_size) for _ in self.checkpoints]\n            \n            # Compute self-influence across all checkpoints\n            for checkpoint_idx, checkpoint in enumerate(self.checkpoints):\n                # Compute gradients for this batch\n                gradients = self.compute_gradients_for_pair(\n                    x1_batch, x2_batch, targets_batch, checkpoint\n                )\n                \n                # Compute gradient norms for each sample in the batch\n                for i in range(current_batch_size):\n                    grad_norm_squared = 0\n                    for grad in gradients:\n                        # Extract gradients for this sample and compute squared norm\n                        grad_norm_squared += (grad[i] ** 2).sum().cpu().numpy()\n                    \n                    # Add to batch self-influence\n                    batch_self_influence[i] += grad_norm_squared\n                    \n                    # Store per-checkpoint self-influence\n                    batch_per_checkpoint_influence[checkpoint_idx][i] = grad_norm_squared\n            \n            # Save self-influence scores\n            for i, idx in enumerate(indices):\n                self_influence_scores[idx] = float(batch_self_influence[i])\n                \n                # Save per-checkpoint self-influence scores\n                per_checkpoint_influences[idx] = {\n                    checkpoint_names[j]: float(batch_per_checkpoint_influence[j][i])\n                    for j in range(len(self.checkpoints))\n                }\n            \n            if batch_idx % 10 == 0:\n                print(f\"Processed {batch_idx+1}/{len(dataloader)} batches\")\n        \n        return self_influence_scores, per_checkpoint_influences\n    \n    def save_to_project_metadata(\n        self,\n        influence_scores: Dict[str, float],\n        metadata_dir: str,\n        material: str,\n        frequency: str,\n        score_name: str = \"tracin_influence\"\n    ) -> None:\n        """\n        Save influence scores to project metadata.\n        \n        This method saves influence scores in the project's metadata directory,\n        using project-specific naming conventions.\n        \n        Args:\n            influence_scores: Dictionary mapping sample IDs to influence scores\n            metadata_dir: Directory to save metadata files\n            material: Material name (e.g., \"plastic\")\n            frequency: Frequency name (e.g., \"500hz\")\n            score_name: Name of the score in the metadata file\n        """\n        # Construct metadata path\n        metadata_path = os.path.join(\n            metadata_dir,\n            f\"{material}_{frequency}_influence_metadata.json\"\n        )\n        \n        # Make directory if it doesn't exist\n        os.makedirs(os.path.dirname(metadata_path), exist_ok=True)\n        \n        # Load existing metadata if it exists\n        metadata = {}\n        if os.path.exists(metadata_path):\n            with open(metadata_path, 'r') as f:\n                metadata = json.load(f)\n        \n        # Update metadata with influence scores\n        for sample_id, scores in influence_scores.items():\n            if sample_id not in metadata:\n                metadata[sample_id] = {}\n            \n            # For scores that are dictionaries themselves, add each score\n            if isinstance(scores, dict):\n                for sub_name, score in scores.items():\n                    metadata[sample_id][f\"{score_name}_{sub_name}\"] = score\n            else:\n                # For single scores, just add the score\n                metadata[sample_id][score_name] = scores\n        \n        # Save updated metadata\n        with open(metadata_path, 'w') as f:\n            json.dump(metadata, f, indent=2)\n        \n        print(f\"Saved influence scores to {metadata_path}\")\n    \n    def save_per_checkpoint_influence(\n        self,\n        per_checkpoint_influences: Dict[str, Dict[str, float]],\n        metadata_dir: str,\n        material: str,\n        frequency: str,\n        score_name: str = \"tracin_influence\"\n    ) -> None:\n        """\n        Save per-checkpoint influence scores to project metadata.\n        \n        This method saves per-checkpoint influence scores in the project's metadata directory,\n        using project-specific naming conventions.\n        \n        Args:\n            per_checkpoint_influences: Dictionary mapping sample IDs to dictionaries of checkpoint-specific influence scores\n            metadata_dir: Directory to save metadata files\n            material: Material name (e.g., \"plastic\")\n            frequency: Frequency name (e.g., \"500hz\")\n            score_name: Name of the score in the metadata file\n        """\n        # Construct metadata path\n        metadata_path = os.path.join(\n            metadata_dir,\n            f\"{material}_{frequency}_per_checkpoint_influence_metadata.json\"\n        )\n        \n        # Make directory if it doesn't exist\n        os.makedirs(os.path.dirname(metadata_path), exist_ok=True)\n        \n        # Load existing metadata if it exists\n        metadata = {}\n        if os.path.exists(metadata_path):\n            with open(metadata_path, 'r') as f:\n                metadata = json.load(f)\n        \n        # Update metadata with per-checkpoint influence scores\n        for sample_id, checkpoint_scores in per_checkpoint_influences.items():\n            if sample_id not in metadata:\n                metadata[sample_id] = {}\n            \n            for checkpoint_name, score in checkpoint_scores.items():\n                metadata[sample_id][f\"{score_name}_{checkpoint_name}\"] = score\n        \n        # Save updated metadata\n        with open(metadata_path, 'w') as f:\n            json.dump(metadata, f, indent=2)\n        \n        print(f\"Saved per-checkpoint influence scores to {metadata_path}\")\n